import { GoogleGenerativeAI } from "@google/generative-ai";
import { NextResponse } from "next/server";
import Replicate from "replicate"; 

const apiKey = process.env.GOOGLE_GENERATIVE_AI_API_KEY || "";
const genAI = new GoogleGenerativeAI(apiKey);

const replicate = new Replicate({
  auth: process.env.REPLICATE_API_TOKEN || "MISSING_KEY",
});

export const runtime = "nodejs"; 
export const maxDuration = 300; 

// 1. GET: è½®è¯¢ä»»åŠ¡
export async function GET(req: Request) {
    const { searchParams } = new URL(req.url);
    const id = searchParams.get('id');
    if (!id) return NextResponse.json({ error: 'Missing ID' }, { status: 400 });
    try {
        const prediction = await replicate.predictions.get(id);
        return NextResponse.json(prediction);
    } catch (e: any) {
        return NextResponse.json({ error: e.message }, { status: 500 });
    }
}

// 2. POST: æäº¤ä»»åŠ¡
export async function POST(req: Request) {
  try {
    const body = await req.json();
    const { messages, model, prompt: videoPrompt, prompt_optimizer, first_frame_image } = body;

    // --- ğŸ¬ è§†é¢‘ä»»åŠ¡ï¼šæ¥å…¥ Minimax Video-01 ---
    if (model === 'sora-v1' || model === 'veo-google') {
        const prediction = await replicate.predictions.create({
            // ä½¿ç”¨ Replicate å®˜æ–¹ Minimax æœ€æ–° Hash
            version: "7660676e1e3985a63974a9d2712812061405788bd98684d03612d7c71aa8d913",
            input: {
                prompt: videoPrompt || "A cinematic scene, high detail",
                prompt_optimizer: prompt_optimizer ?? true,
                first_frame_image: first_frame_image || undefined
            }
        });
        return NextResponse.json({ type: 'async_job', id: prediction.id, status: prediction.status });
    }

    // --- ğŸ¨ å›¾ç‰‡ä»»åŠ¡ï¼šBanana SDXL ---
    if (model === 'banana-sdxl') {
        const lastPrompt = messages[messages.length-1].content.text;
        const output: any = await replicate.run(
          "stability-ai/sdxl:39ed52f2a78e934b3ba6e2a89f5b1c712de7dfea535525255b1aa35c5565e08b",
          { input: { prompt: lastPrompt, width: 1024, height: 1024 } }
        );
        return NextResponse.json({ url: output[0] });
    }

    // --- ğŸ§  èŠå¤©ï¼šä¿®å¤ 503 Overload æ˜ å°„ ---
    // ğŸš¨ å¼ºåˆ¶æ¨¡å‹æ˜ å°„ï¼šå°† UI çš„ 2.5 æ˜ å°„åˆ°å¯ç”¨çš„ 1.5 Pro
    let validModel = "gemini-1.5-flash";
    if (model.includes("pro") || model.includes("2.5") || model.includes("exp")) {
        validModel = "gemini-1.5-pro"; 
    }

    const geminiModel = genAI.getGenerativeModel({ model: validModel });
    const chat = geminiModel.startChat({
        history: messages.slice(0, -1).map((m: any) => ({
            role: m.role === 'user' ? 'user' : 'model',
            parts: [{ text: typeof m.content === 'string' ? m.content : m.content.text }],
        }))
    });

    const chatContent = typeof messages[messages.length - 1].content === 'string' 
        ? messages[messages.length - 1].content 
        : messages[messages.length - 1].content.text;

    const result = await chat.sendMessageStream(chatContent);

    const stream = new ReadableStream({
      async start(controller) {
        for await (const chunk of result.stream) {
          const text = chunk.text();
          if (text) controller.enqueue(new TextEncoder().encode(text));
        }
        controller.close();
      },
    });
    return new Response(stream);

  } catch (error: any) {
    console.error("API Error:", error);
    return NextResponse.json({ error: "æœåŠ¡å™¨è´Ÿè½½ä¸­ï¼Œè¯·åˆ·æ–°é‡è¯•" }, { status: 503 });
  }
}